{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Optimize a grid of your choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline consists of three main stages:\n",
    "\n",
    "1. **Data Importing and Preprocessing**  \n",
    "    This stage involves loading your grid data and preparing it for optimization. Preprocessing may include cleaning the data, handling unsupported elements, and performing calculations such as busbar outage analysis.\n",
    "\n",
    "2. **DC Optimization**  \n",
    "    In this stage, the pipeline runs optimization (Map Elites) algorithm using GPU accelerated DC Load flow solver on the preprocessed grid data. The optimizer uses specified metrics and parameters to search for optimal grid topologies or configurations.\n",
    "\n",
    "3. **AC Validation**  \n",
    "    After optimization, the best solutions are validated using AC (Alternating Current) power flow analysis to ensure feasibility and performance under more realistic conditions.\n",
    "\n",
    "---\n",
    "\n",
    "Before running the pipeline on your data, you need to configure the following:\n",
    "\n",
    "1. **PipelineConfig**  \n",
    "    Define the experiment name (`iteration_name`) and the grid file name (`file_name`). The experiment name determines the folder where results and intermediate files are stored. The grid file should be placed inside this folder and can be in formats such as `.xiidm`, `.json` (for PandaPower), or `.zip` (for CGMES).\n",
    "\n",
    "    Note that a folder named 'iteration_name' should be created inside the 'data' folder and the grid file named 'file_name' should be put inside the 'iteration_name' folder.\n",
    "\n",
    "2. **Importer Config**  \n",
    "    Set parameters for importing the grid data, such as area settings and other options relevant to your data source. This configuration ensures that the importer correctly interprets and processes your grid file.\n",
    "\n",
    "3. **Preprocessing Parameters**  \n",
    "    Specify options for the preprocessing step, which are independent of the data source. For example, you can enable or disable preprocessing for busbar outage calculations or set limits for action sets.\n",
    "\n",
    "4. **DC Optimization Configuration (`dc_optimization_cfg`)**  \n",
    "    Define the parameters for the optimizer and loadflow solver. This includes settings such as which metrics to use in the objective function, runtime limits, the number of worst contingencies to consider, and other optimizer-specific options.\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow Summary:**\n",
    "\n",
    "- Set up the required configurations as described above.\n",
    "- Run the pipeline to import, preprocess, and optimize your grid data.\n",
    "- Optionally, rerun the optimization with different parameters without repeating preprocessing or optimisation.\n",
    "- Validate the optimized solutions using AC power flow analysis.\n",
    "\n",
    "This modular approach allows you to efficiently experiment with different configurations and optimization strategies while minimizing redundant computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toop_engine_topology_optimizer.benchmark.benchmark_utils import *\n",
    "logger.enable() # Enable logger of benchmark_utils\n",
    "logger.level = logbook.INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Reminder:**  \n",
    "> Create a folder named `iteration_name` inside the `data` folder, and place the grid file named `file_name` inside the `iteration_name` folder.  \n",
    ">  \n",
    "> For example:  \n",
    "> ```\n",
    "> data/\n",
    "> └── <iteration_name>/\n",
    ">     └── <file_name>\n",
    "> ```\n",
    "\n",
    "The data folder already exists in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline configuration. Here iteration_name corresponds to the name of the experiment. For example, \"test_tso\". This folder has to be created inside the \"data\" folder. This folder is supposed to contain the grid file specified in file_name.\n",
    "# file_name corresponds to the name of the grid file. This file has to be present inside the folder specified by iteration_name. This folder can have grid data from multiple timesteps.\n",
    "# The grid file can be in any of the supported formats: .xiidm, .json, .mat, .zip (in case of cgmes)\n",
    "\n",
    "iteration_name = \"\" # Change if you use several iterations\n",
    "file_name = \"grid.xiidm\" # Path to the grid file\n",
    "\n",
    "pipeline_cfg = PipelineConfig(\n",
    "    root_path=Path(\"../data/complex_grid\"),\n",
    "    iteration_name = iteration_name,\n",
    "    file_name = file_name,\n",
    "    grid_type=\"powsybl\")\n",
    "\n",
    "# Set up dc_optimisation_configs\n",
    "iteration_path, file_path, data_folder, optimizer_snapshot_dir = get_paths(pipeline_cfg)\n",
    "static_information_file = data_folder / pipeline_cfg.static_info_relpath\n",
    "\n",
    "# Example configuration for the optimizer\n",
    "# Note that this is a minimal example; adjust as needed.\n",
    "# Look at topology_optimizer.interfaces.messages.dc_params.BatchedMEParameters and topology_optimizer.interfaces.messages.dc_params.LoadflowSolverParameters for options.\n",
    "# The values for these configuration classes are set in the \"lf_config\" and \"ga_config\" sections below.\n",
    "dc_optimization_cfg = {\n",
    "    \"task_name\": \"test\",\n",
    "    \"fixed_files\": [str(static_information_file)],\n",
    "    \"double_precision\": None,\n",
    "    \"tensorboard_dir\": str(iteration_path) + \"/results/{task_name}\",\n",
    "    \"stats_dir\": str(iteration_path) + \"/results/{task_name}\",\n",
    "    \"summary_frequency\": None,\n",
    "    \"checkpoint_frequency\": None,\n",
    "    \"stdout\": None,\n",
    "    \"double_limits\": None,\n",
    "    \"num_cuda_devices\": 1,\n",
    "    \"omp_num_threads\": 1,\n",
    "    \"xla_force_host_platform_device_count\": None,\n",
    "    \"output_json\": str(iteration_path) + \"/results/output.json\",\n",
    "    \"lf_config\": {\"distributed\": False},\n",
    "    \"ga_config\": {\n",
    "        \"runtime_seconds\": 30,\n",
    "        \"me_descriptors\": [{\"metric\": \"split_subs\", \"num_cells\": 10}],\n",
    "        \"observed_metrics\": [\"overload_energy_n_1\", \"split_subs\"],\n",
    "        \"n_worst_contingencies\": 2\n",
    "    },\n",
    "}\n",
    "\n",
    "logger.info(\"Preparing importer parameters...\")\n",
    "importer_parameters = prepare_importer_parameters(file_path, data_folder)\n",
    "importer_parameters.area_settings.cutoff_voltage = 10\n",
    "\n",
    "logger.info(\"Setting up preprocessing parameters...\")\n",
    "preprocessing_parameters = PreprocessParameters(\n",
    "    action_set_clip=2**10,\n",
    "    enable_bb_outage=True,\n",
    "    bb_outage_as_nminus1=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    topo_paths = run_pipeline(pipeline_cfg=pipeline_cfg, dc_optim_config=dc_optimization_cfg, importer_parameters=importer_parameters, preprocessing_parameters=preprocessing_parameters)\n",
    "    logger.info(f\"Pipeline finished in {time.time() - start:.2f} seconds\")\n",
    "except Exception as exc:\n",
    "    logger.exception(f\"Pipeline failed: {exc}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results generated by the pipeline are organized in a hierarchical folder structure within the experiment directory (e.g., `data/rte/20250220T0830`). This structure is designed to facilitate easy access to all intermediate and final outputs of the optimization and validation process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Folder Structure Overview**\n",
    "\n",
    "```\n",
    "data/\n",
    "└── <iteration_name>/                # e.g., rte\n",
    "    |── <file_name_without_ext>/     # e.g., 20250212T0830\n",
    "    |    ├── optimizer_snapshot/\n",
    "    |    │   ├── run_1/\n",
    "    |    │   │   ├── res.json\n",
    "    |    │   │   ├── topology_0/\n",
    "    |    │   │   │   ├── modified_network.xiidm\n",
    "    |    │   │   │   ├── ac_loadflow_results.csv\n",
    "    |    │   │   │   ├── dc_loadflow_results.csv\n",
    "    |    │   │   │   ├── sld\n",
    "    |    │   │   │   │   ├── sld_split_station_1.png\n",
    "    |    │   │   │   │   ├── sld_split_station_2.png\n",
    "    |    │   │   │   │   └── ...\n",
    "    |    │   │   ├── topology_1/\n",
    "    |    │   │   └── ...\n",
    "    |    │   └── ...\n",
    "    |    ├── run_2/\n",
    "    |    │   └── ...\n",
    "    |    └── ...\n",
    "    |    └── ...\n",
    "    └── results/\n",
    "        └── task_name/\n",
    "            └── ...\n",
    "```\n",
    "#### **optimizer_snapshot/**\n",
    "- This folder contains the results of all optimization runs for the given experiment.\n",
    "- Each optimization run is stored in a separate subfolder (e.g., `run_1`, `run_2`, etc.). The names of these folders correspond to different executions of the optimizer, which may vary based on parameters or random seeds.\n",
    "\n",
    "#### **optimizer_snapshot/run_{n}/**\n",
    "- Each `run_{n}` folder corresponds to a single execution of the optimizer with a specific configuration.\n",
    "- **res.json**:  \n",
    "  - This file summarizes the results of the optimization run, including metrics, best topologies found, and references to the evaluated topologies.\n",
    "- **topology_{x}/**:  \n",
    "  - Each subfolder represents a specific topology evaluated during the AC validation stage.\n",
    "  - `{x}` is the index of the topology (e.g., `topology_0`, `topology_1`, etc.).\n",
    "\n",
    "#### **optimizer_snapshot/run_{n}/topology_{x}/**\n",
    "- **modified_network.xiidm**:  \n",
    "  - The grid file representing the network after applying the topology changes for this specific solution.\n",
    "- **ac_loadflow_results.csv**:  \n",
    "  - Results of the AC loadflow analysis for this topology containing both the N-0 and N-1 scenarios.\n",
    "- **dc_loadflow_results.csv**:  \n",
    "  - Results of the DC loadflow analysis for this topology.\n",
    "- **sld/{y}_sld.png**:  \n",
    "  - Single line diagrams (SLDs) for substations that were split as part of the topology modification.  \n",
    "  - `{y}` corresponds to the name of the split station.\n",
    "\n",
    "#### **results/task_name/**\n",
    "- This folder is reserved for storing the checkpoint results and also the tensorboard logs for each DC optimisation task, organized by task name. Note that the 'task_name' corresponds to the name of the task defined in the `dc_optimisation_config`.\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **optimizer_snapshot**: Contains all optimization runs.\n",
    "- **run_{n}**: Each run folder contains results for a specific optimizer execution.\n",
    "- **res.json**: Summary of the run and references to topologies.\n",
    "- **topology_{x}**: Contains all files related to a specific evaluated topology, including the modified grid, loadflow results, and diagrams.\n",
    "- **results/task_name**: Contains checkpoint results and logs for each DC optimization task.\n",
    "\n",
    "This structure ensures that all results are logically grouped and easily accessible for further analysis, visualization, or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you wish to run optimisation again without re-running the preprocessing steps,\n",
    "maybe with a different optimisation configuration, you can do so by setting\n",
    "run_preprocessing_stage to False. You can save time by reusing the preprocessed data\n",
    "from the previous pipeline run.\n",
    "\"\"\"\n",
    "\n",
    "# Note that you can change the dc_optimisation_task_name to avoid overwriting the previous results.\n",
    "# Here we change it to \"test-2\"\n",
    "dc_optimization_cfg = {\n",
    "    \"task_name\": \"test-2\",\n",
    "    \"fixed_files\": [str(static_information_file)],\n",
    "    \"double_precision\": None,\n",
    "    \"tensorboard_dir\": str(iteration_path) + \"/results/{task_name}\",\n",
    "    \"stats_dir\": str(iteration_path) + \"/results/{task_name}\",\n",
    "    \"summary_frequency\": None,\n",
    "    \"checkpoint_frequency\": None,\n",
    "    \"stdout\": None,\n",
    "    \"double_limits\": None,\n",
    "    \"num_cuda_devices\": 1,\n",
    "    \"omp_num_threads\": 1,\n",
    "    \"xla_force_host_platform_device_count\": None,\n",
    "    \"output_json\": str(iteration_path) + \"/results/output.json\",\n",
    "    \"lf_config\": {\"distributed\": False},\n",
    "    \"ga_config\": {\n",
    "        \"runtime_seconds\": 30,\n",
    "        \"me_descriptors\": [{\"metric\": \"split_subs\", \"num_cells\": 10}],\n",
    "        \"observed_metrics\": [\"overload_energy_n_1\", \"split_subs\"],\n",
    "        \"n_worst_contingencies\": 2\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "topo_paths = run_pipeline(pipeline_cfg=pipeline_cfg, dc_optim_config=dc_optimization_cfg, importer_parameters=importer_parameters, preprocessing_parameters=preprocessing_parameters, run_preprocessing_stage=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "If you wish to perform AC validation for a particular run directory, you can do so by setting\n",
    "run_optimization_stage to False and providing the run directory path via the optimisation_run_dir parameter.\n",
    "Make sure that the provided directory contains a valid res.json file.\n",
    "\n",
    "Note that by default the 5 best topologies in the list of best topologies will be used for AC validation.\n",
    "If you want to use a different number of topologies, you can modify the k_best_topos argument of the run_pipeline function.\n",
    "\"\"\"\n",
    "\n",
    "# Hint: optimisation_run_dir should contain the path to the folder which contains the valid res.json file.\n",
    "optimisation_run_dir = f\"../data/{optimizer_snapshot_dir}/run_0\"\n",
    "optimisation_run_dir = Path(optimisation_run_dir)\n",
    "\n",
    "topo_paths = run_pipeline(pipeline_cfg=pipeline_cfg,\n",
    "             dc_optim_config=dc_optimization_cfg,\n",
    "             importer_parameters=importer_parameters,\n",
    "             preprocessing_parameters=preprocessing_parameters,\n",
    "             run_preprocessing_stage=False,\n",
    "             run_optimization_stage=False,\n",
    "             run_ac_validation_stage=True,\n",
    "             optimisation_run_dir=optimisation_run_dir,\n",
    "             k_best_topos=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the grid with powsybl's network explorer\n",
    "\n",
    "Use the next cell to interactively explore the grid topology and results using pypowsybl's network explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypowsybl as pp\n",
    "from pypowsybl_jupyter import network_explorer\n",
    "\n",
    "original_net_path = file_path # Path to the original network file\n",
    "\n",
    "net = pp.network.load(str(original_net_path))\n",
    "pp.loadflow.run_ac(net)\n",
    "component_library = \"Convergence\"\n",
    "sld_param = pp.network.SldParameters(use_name=True, component_library = component_library, nodes_infos=True, display_current_feeder_info = True)\n",
    "nad_parameters=pp.network.NadParameters(edge_info_along_edge=True, substation_description_displayed=True)\n",
    "network_explorer(net, depth=0, sld_parameters=sld_param, nad_parameters=nad_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs:\n",
    "# Write tests for the helper functions\n",
    "# Adapt the code to support pandapower grid files.\n",
    "# Add support for ieee test grids\n",
    "# Optimise the pipeline code so that data loading doesn't have to be repeated multiple times.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

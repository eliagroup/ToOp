"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.message
import sys

if sys.version_info >= (3, 8):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class StoredLoadflowReference(google.protobuf.message.Message):
    """A reference to a stored loadflow result on disk or in an object store."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    RELATIVE_PATH_FIELD_NUMBER: builtins.int
    relative_path: builtins.str
    """Loadflow results are too large to be sent directly over kafka, so they need to be stored somewhere and referenced.
    They are stored and written using the functions in `loadflow_result_helpers_new.py`, which use the `fsspec` library
    to abstract away the filesystem. Hence, these can write to local disk, Azure bucket, ...

    The reference contains the filename relative to the base path or bucket defined in the filesystem, i.e. if a
    DirFileSystem is used with base_path="/path/to/base" and the filename is "loadflows" then the full path is
    "/path/to/base/loadflows/node_results.parquet", "/path/to/base/loadflows/branch_results.parquet", ...
    The folder of the loadflow result relative to the base path or bucket in the filesystem. This points to a directory
    under which the files "node_results.parquet", "branch_results.parquet", "metadata.json", ... are stored.
    """
    def __init__(
        self,
        *,
        relative_path: builtins.str = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["relative_path", b"relative_path"]) -> None: ...

global___StoredLoadflowReference = StoredLoadflowReference
